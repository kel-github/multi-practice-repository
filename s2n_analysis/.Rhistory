#   # implement Hick-Hyman's law for a decision involving 2 events, p = p of event 1
#   sum(  p * log2( (1/p) + 1 ),  (1-p) * log2( (1/( 1- p)) + 1   ) )
# }
generate.noise <- function(n.feat, n.stim){
# n.feat = the number of features that require noise assigned to them
# n.stim = the total number of stimuli sampled across the two tasks
# mat.dims = the dimensions of the n x p internal stimulus matrix
# output is a matrix of noise to be added to the stimuli encoded matrix
matrix(rnorm(n.feat*n.stim, mean=0, sd=.1), nrow=n.feat)
}
run.sim <- function(n.stim, n.task, n.vars, n.practice.trials, priors, stim.mat, n.features, learn.rate, cov.fact, stim.overlap){
# initialise dataframe to collect behavioural results
kl.dat = data.frame(covar=factor(rep(cov.fact, each=n.practice.trials)),
trial=factor(c(1:n.practice.trials)),
KL=rep(NA, times=n.practice.trials),
interference.T=rep(NA, times=n.practice.trials))
task.a.priors = priors # initialise task A model parameters
task.b.priors = priors  # same for task B
# set the range of values to sample stimuli from for each task
irrel.idxs = ((n.task*n.stim)+1):ncol(stim.mat)
task.idxs = matrix(c(1:(n.stim*n.task)), byrow=FALSE, nrow=n.stim)
if (stim.overlap == "low") {
task.idxs[sample(1:nrow(task.idxs), 5),1] = task.idxs[sample(1:nrow(task.idxs), 5), 2]
} else if (stim.overlap == "high") {
task.idxs[,1] = task.idxs[, 2]
}
# now run the simulation
for (i in 1:n.practice.trials){
# step 1: select stimuli from the set of task A, from the set of task B, and if an even trial, from the set of task C
if (stim.overlap == "none"){
n = 1
} else {
n = 2
}
trial.idxs = apply(task.idxs, 2, sample, n)
# if an even trial, add a coincidental stimulus
if (i%%2 == 0) trial.idxs = rbind(trial.idxs, rep(sample(irrel.idxs, 1), times=n.task))
trial.idxs = as.vector(trial.idxs)
# encoded stimulu
stim.encoded = stim.mat[,trial.idxs] + generate.noise(n.features, length(trial.idxs))
stim.encoded = apply(stim.encoded, 2, norm.vec.lngth)
distance = apply(stim.encoded, 2, function(x) apply(stim.mat, 2, calc.euclid, y = x))
similarity = 1/distance
# update priors (from 1st half of sim mat columns for task A, second half for task B)
if (n.task == 2 & i%%2 != 0 & stim.overlap == "none"){
task.a.priors = task.a.priors + (learn.rate * similarity[,1])
task.b.priors = task.b.priors + (learn.rate * similarity[,2])
} else {
task.a.priors = task.a.priors + (learn.rate * apply(similarity[,1:(length(trial.idxs)/2)], 1, sum ))
task.b.priors = task.b.priors + (learn.rate * apply(similarity[,((length(trial.idxs)/2)+1):length(trial.idxs)], 1, sum ))
}
#   prior.dat$task.A[prior.dat$trial==i] = task.a.priors
#   prior.dat$task.B[prior.dat$trial==i] = task.b.priors
kl.dat$KL[i] = get.KL(task.b.priors, task.a.priors)
kl.dat$interference.T[i] = 1/kl.dat$KL[i]
}
kl.dat
}
n.stim = 10
n.task = 2
n.features = 10 # randomly chosen
n.practice.trials = 15
priors = rep(1, times=(n.stim*n.task)+n.stim) # priors over each the task space (to define Dirichlet distribution)
learn.rate = 0.01 # a weighting for new observations
cov.rngs = list(lo=c(-1, 1),hi=c(9, 11))
cov.fact=c("lo", "hi")
n.vars = length(cov.rngs)
n.sims = 1000 # number of simulations to perform
cov.mat = lapply(cov.rngs, gen.cov.mat, n.stim=(n.stim*n.task)+n.stim) # generate a covariance matrix from which to generate random, correlated vectors that is n = total stim across tasks, plus n.stim number of coincidental stimuli
cov.mat = lapply(cov.mat, make.positive.definite, tol=1e-3) # using this to make matrix positive definite, on account of advice from https://stackoverflow.com/questions/27176595/error-sigma-must-be-positive-definite/27179588
# if required, make positive definite
s.mat = lapply(cov.mat, mvrnorm, n=n.features, mu=rep(0, times=(n.stim*n.task)+n.stim))
# check the stim.mat covariance matrix is similar to the input one
check = cov(s.mat$lo) # visual check is comforting
# now normalise the vectors to unit length (for using distance measure below)
s.mat = lapply(s.mat, function(x) apply(x, 2, norm.vec.lngth))
# first, generate the stimulus based on one of the existing stimulus vectors, add noise
this.stim.idx = sample(1:n.stim*n.task, 1) # which stimulus is presented?
stim.encoded = s.mat$hi[,this.stim.idx] + rnorm(n.features, mean=0, sd=.1)
stim.encoded = norm.vec.lngth(stim.encoded)
distance = apply(s.mat$hi, 2, calc.euclid, y = stim.encoded)
similarity = 1/distance
kl.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task, n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features, learn.rate=learn.rate, stim.overlap = "none"), SIMPLIFY=FALSE))
kl.dat = do.call(rbind, kl.dat)
kl.dat = kl.dat %>% group_by(covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
kl.dat
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 4000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 5000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
overlap.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "low"), SIMPLIFY=FALSE))
overlap.dat = do.call(rbind, overlap.dat)
overlap.dat$overlap = "low"
nu.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "high"), SIMPLIFY=FALSE))
nu.dat = do.call(rbind, nu.dat)
nu.dat$overlap = "high"
overlap.dat = rbind(overlap.dat, nu.dat)
rm(nu.dat)
overlap.dat$overlap <- as.factor(overlap.dat$overlap)
levels(overlap.dat$overlap) = c("low", "high")
overlap.dat = overlap.dat %>% group_by(overlap, covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
overlap.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1.5) +
ylim(10, 5000) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.4, colour=NA) +
facet_wrap(~ overlap) +
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
rm(list=ls())
library(tidyverse)
library(rmarkdown)    # You need this library to run this template.
library(epuRate)      # Install with devtools: install_github("holtzy/epuRate", force=TRUE)
library(MASS)
library(corpcor)
library(wesanderson)
gen.cov.mat <- function(n.stim, cov.rng){
# generates covariance matrix to initialise vectors
# output = mat (an n.stim x n.stim covariance matrix)
# inputs =
# n.stim - the number of stimuli, covariance matrix will be n x n
# cor.rng = lower and upper bound of task covariances
mat = diag(x=1, n.stim, n.stim)
cor.rng <- runif(length(mat[lower.tri(mat)]), min(cov.rng), max(cov.rng))
mat[lower.tri(mat)] = cor.rng
mat = t(mat)
mat[lower.tri(mat)] = cor.rng
mat
}
norm.vec.lngth <- function(v){
# this function takes a vector and returns the unit vector
mu = 1/ sqrt( t(v) %*% v ) # first is transferred as inputs are interpreted as row vectors
v = mu * v
v
}
calc.euclid <- function(x,y){
# compute the euclidean distance between two vectors
sqrt( sum( (x-y) ^ 2 )  )
}
rdirichlet<-function (n, alpha) {
# I got this function from:
# https://stats.stackexchange.com/questions/166289/generate-data-from-dirichlet-distribution - also see page 585 of Gelman's BDA, 3r ed.
l <- length(alpha)
x <- matrix(rgamma(l * n, alpha), ncol = l, byrow = TRUE) # generate n observations using the gamma function, given l alpha paramaters, and n observations
sm <- x %*% rep(1, l) # sum each row of the matrix
x/as.vector(sm)
}
get.KL <- function(alpha, beta){
# find out how well the dirichlet distribution defined by alpha is approximated by the dirichlet distribution defined by the beta parameters. Computes the KL divergence using the derivation at:
# http://bariskurt.com/kullback-leibler-divergence-between-two-dirichlet-and-beta-distributions/
lgamma(sum(alpha)) - lgamma(sum(beta)) - sum(lgamma(alpha)) + sum(lgamma(beta)) + sum((alpha - beta)*(digamma(alpha)-digamma(sum(beta))))
}
# get.interference <- function(p){
#   # implement Hick-Hyman's law for a decision involving 2 events, p = p of event 1
#   sum(  p * log2( (1/p) + 1 ),  (1-p) * log2( (1/( 1- p)) + 1   ) )
# }
generate.noise <- function(n.feat, n.stim){
# n.feat = the number of features that require noise assigned to them
# n.stim = the total number of stimuli sampled across the two tasks
# mat.dims = the dimensions of the n x p internal stimulus matrix
# output is a matrix of noise to be added to the stimuli encoded matrix
matrix(rnorm(n.feat*n.stim, mean=0, sd=.1), nrow=n.feat)
}
run.sim <- function(n.stim, n.task, n.vars, n.practice.trials, priors, stim.mat, n.features, learn.rate, cov.fact, stim.overlap){
# initialise dataframe to collect behavioural results
kl.dat = data.frame(covar=factor(rep(cov.fact, each=n.practice.trials)),
trial=factor(c(1:n.practice.trials)),
KL=rep(NA, times=n.practice.trials),
interference.T=rep(NA, times=n.practice.trials))
task.a.priors = priors # initialise task A model parameters
task.b.priors = priors  # same for task B
# set the range of values to sample stimuli from for each task
irrel.idxs = ((n.task*n.stim)+1):ncol(stim.mat)
task.idxs = matrix(c(1:(n.stim*n.task)), byrow=FALSE, nrow=n.stim)
if (stim.overlap == "low") {
task.idxs[sample(1:nrow(task.idxs), 5),1] = task.idxs[sample(1:nrow(task.idxs), 5), 2]
} else if (stim.overlap == "high") {
task.idxs[,1] = task.idxs[, 2]
}
# now run the simulation
for (i in 1:n.practice.trials){
# step 1: select stimuli from the set of task A, from the set of task B, and if an even trial, from the set of task C
if (stim.overlap == "none"){
n = 1
} else {
n = 2
}
trial.idxs = apply(task.idxs, 2, sample, n)
# if an even trial, add a coincidental stimulus
if (i%%2 == 0) trial.idxs = rbind(trial.idxs, rep(sample(irrel.idxs, 1), times=n.task))
trial.idxs = as.vector(trial.idxs)
# encoded stimulu
stim.encoded = stim.mat[,trial.idxs] + generate.noise(n.features, length(trial.idxs))
stim.encoded = apply(stim.encoded, 2, norm.vec.lngth)
distance = apply(stim.encoded, 2, function(x) apply(stim.mat, 2, calc.euclid, y = x))
similarity = 1/distance
# update priors (from 1st half of sim mat columns for task A, second half for task B)
if (n.task == 2 & i%%2 != 0 & stim.overlap == "none"){
task.a.priors = task.a.priors + (learn.rate * similarity[,1])
task.b.priors = task.b.priors + (learn.rate * similarity[,2])
} else {
task.a.priors = task.a.priors + (learn.rate * apply(similarity[,1:(length(trial.idxs)/2)], 1, sum ))
task.b.priors = task.b.priors + (learn.rate * apply(similarity[,((length(trial.idxs)/2)+1):length(trial.idxs)], 1, sum ))
}
#   prior.dat$task.A[prior.dat$trial==i] = task.a.priors
#   prior.dat$task.B[prior.dat$trial==i] = task.b.priors
kl.dat$KL[i] = get.KL(task.b.priors, task.a.priors)
kl.dat$interference.T[i] = 1/kl.dat$KL[i]
}
kl.dat
}
n.stim = 10
n.task = 2
n.features = 10 # randomly chosen
n.practice.trials = 10
priors = rep(1, times=(n.stim*n.task)+n.stim) # priors over each the task space (to define Dirichlet distribution)
learn.rate = 0.01 # a weighting for new observations
cov.rngs = list(lo=c(-1, 1),hi=c(9, 11))
cov.fact=c("lo", "hi")
n.vars = length(cov.rngs)
n.sims = 1000 # number of simulations to perform
cov.mat = lapply(cov.rngs, gen.cov.mat, n.stim=(n.stim*n.task)+n.stim) # generate a covariance matrix from which to generate random, correlated vectors that is n = total stim across tasks, plus n.stim number of coincidental stimuli
cov.mat = lapply(cov.mat, make.positive.definite, tol=1e-3) # using this to make matrix positive definite, on account of advice from https://stackoverflow.com/questions/27176595/error-sigma-must-be-positive-definite/27179588
# if required, make positive definite
s.mat = lapply(cov.mat, mvrnorm, n=n.features, mu=rep(0, times=(n.stim*n.task)+n.stim))
# check the stim.mat covariance matrix is similar to the input one
check = cov(s.mat$lo) # visual check is comforting
# now normalise the vectors to unit length (for using distance measure below)
s.mat = lapply(s.mat, function(x) apply(x, 2, norm.vec.lngth))
# first, generate the stimulus based on one of the existing stimulus vectors, add noise
this.stim.idx = sample(1:n.stim*n.task, 1) # which stimulus is presented?
stim.encoded = s.mat$hi[,this.stim.idx] + rnorm(n.features, mean=0, sd=.1)
stim.encoded = norm.vec.lngth(stim.encoded)
distance = apply(s.mat$hi, 2, calc.euclid, y = stim.encoded)
similarity = 1/distance
kl.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task, n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features, learn.rate=learn.rate, stim.overlap = "none"), SIMPLIFY=FALSE))
kl.dat = do.call(rbind, kl.dat)
kl.dat = kl.dat %>% group_by(covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 5000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
overlap.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "low"), SIMPLIFY=FALSE))
overlap.dat = do.call(rbind, overlap.dat)
overlap.dat$overlap = "low"
nu.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "high"), SIMPLIFY=FALSE))
nu.dat = do.call(rbind, nu.dat)
nu.dat$overlap = "high"
overlap.dat = rbind(overlap.dat, nu.dat)
rm(nu.dat)
overlap.dat$overlap <- as.factor(overlap.dat$overlap)
levels(overlap.dat$overlap) = c("low", "high")
overlap.dat = overlap.dat %>% group_by(overlap, covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
overlap.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1.5) +
ylim(10, 6000) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.4, colour=NA) +
facet_wrap(~ overlap) +
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
warnings()
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6500) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
randperm(3)
sample(3)
sample(3,1)
rm(list=ls())
## written by K. Garner, 2020
# this is to plot the standard deviations of the timecourses, from the regions
# of interest in the left and right hemipheres of Garner et al 2020 https://doi.org/10.1101/564450
rm(list=ls())
rm(list=ls())
# load required packages and plotting resources
# --------------------------------------------------------------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(cowplot)
library(wesanderson)
library(BayesFactor)
source("R_rainclouds.R")
# define functions
# --------------------------------------------------------------------------------
make.plot <- function(data){
ggplot(data, aes(x=region, y=std, fill = region, colour = region)) +
geom_flat_violin(position = position_nudge(x = .25, y = 0), adjust =2, trim =
TRUE, alpha = 0.6) +
geom_point(position = position_jitter(width = .15), size = .25) +
geom_boxplot(aes(x = region, y = std), outlier.shape = NA,
alpha = 0.3, width = .1, colour = "BLACK") +
scale_color_manual(values=wes_palette("IsleofDogs1")) +
scale_fill_manual(values=wes_palette("IsleofDogs1")) +
ylab('std') + xlab('ROI') + theme_cowplot() +
facet_wrap(~hemisphere) +
guides(fill = FALSE, colour = FALSE) +
theme(axis.text.x = element_text(face = "italic"))
}
z.test <- function(x, y, name){
# compute z test, return statistic value and a 2 sided p-value
z <- list(region = name, z = NA, p = NA)
z$z <- (mean(x) - mean(y)) / sqrt( (sd(x)/sqrt(length(x)))  + (sd(y)/sqrt(length(y))) )
z$p <- 2*pnorm(-abs(z$z))
z
}
# define file variables
# --------------------------------------------------------------------------------
data.dir = 'processed-data'
LH.dat.csv = 's1_LH.csv'
RH.dat.csv = 's1_RH.csv'
roi_names = c("IPS", "Put", "SMA")
save.name = 'tc-std-s1'
# data wrangle
# --------------------------------------------------------------------------------
LH = read.csv(paste('../', data.dir, '/', LH.dat.csv, sep=""), header=F)
names(LH) = roi_names
LH$hemisphere = "left"
RH = read.csv(paste('../', data.dir, '/', RH.dat.csv, sep=""), header=F)
names(RH) = roi_names
RH$hemisphere = "right"
LH = LH %>% pivot_longer(c('IPS', 'Put', 'SMA'), names_to = "region", values_to="std")
RH = RH %>% pivot_longer(c('IPS', 'Put', 'SMA'), names_to = "region", values_to="std")
dat = rbind(LH, RH)
# make plot and save
# --------------------------------------------------------------------------------
p <- make.plot( dat )
ggsave(paste(save.name, '.pdf', sep=''), plot = p, width=10, height=10, units="cm")
ggsave(paste(save.name, '.png', sep=''), plot = p, width=10, height=10, units="cm")
# conduct z-tests on each region and print output
# --------------------------------------------------------------------------------
reg = c("IPS", "Put", "SMA")
lapply(reg, function(x) with(dat, z.test(std[hemisphere == "left" & reg == x],
std[hemisphere == "right" & reg == x],
x)))
# s1 data = IPS: z = -0.2, p = .844, Put: z = 0.12, p = .91, SMA: z = .02, p = .98
# s2_SING = IPS: z = -0.3, p = .78, Put: z = .05, p = .96, SMA: z = -0.01, p = .99
# s2_MULT = IPS: z = -0.28, p = .77, Put: z = .05, p = .96, SMA: z = -0.04, p = .97
rm(list=ls())
# load required packages and plotting resources
# --------------------------------------------------------------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(cowplot)
library(wesanderson)
library(BayesFactor)
source("R_rainclouds.R")
# define functions
# --------------------------------------------------------------------------------
make.plot <- function(data){
ggplot(data, aes(x=region, y=std, fill = region, colour = region)) +
geom_flat_violin(position = position_nudge(x = .25, y = 0), adjust =2, trim =
TRUE, alpha = 0.6) +
geom_point(position = position_jitter(width = .15), size = .25) +
geom_boxplot(aes(x = region, y = std), outlier.shape = NA,
alpha = 0.3, width = .1, colour = "BLACK") +
scale_color_manual(values=wes_palette("IsleofDogs1")) +
scale_fill_manual(values=wes_palette("IsleofDogs1")) +
ylab('std') + xlab('ROI') + theme_cowplot() +
facet_wrap(~hemisphere) +
guides(fill = FALSE, colour = FALSE) +
theme(axis.text.x = element_text(face = "italic"))
}
z.test <- function(x, y, name){
# compute z test, return statistic value and a 2 sided p-value
z <- list(region = name, z = NA, p = NA)
z$z <- (mean(x) - mean(y)) / sqrt( (sd(x)/sqrt(length(x)))  + (sd(y)/sqrt(length(y))) )
z$p <- 2*pnorm(-abs(z$z))
z
}
# define file variables
# --------------------------------------------------------------------------------
data.dir = 'processed-data'
LH.dat.csv = 's2_LH_SING.csv'
RH.dat.csv = 's2_RH_SING.csv'
roi_names = c("IPS", "Put", "SMA")
save.name = 'tc-std-s1s2-sing'
# data wrangle
# --------------------------------------------------------------------------------
LH = read.csv(paste('../', data.dir, '/', LH.dat.csv, sep=""), header=F)
names(LH) = roi_names
LH$hemisphere = "left"
RH = read.csv(paste('../', data.dir, '/', RH.dat.csv, sep=""), header=F)
names(RH) = roi_names
RH$hemisphere = "right"
LH = LH %>% pivot_longer(c('IPS', 'Put', 'SMA'), names_to = "region", values_to="std")
RH = RH %>% pivot_longer(c('IPS', 'Put', 'SMA'), names_to = "region", values_to="std")
dat = rbind(LH, RH)
# make plot and save
# --------------------------------------------------------------------------------
p <- make.plot( dat )
ggsave(paste(save.name, '.pdf', sep=''), plot = p, width=10, height=10, units="cm")
ggsave(paste(save.name, '.png', sep=''), plot = p, width=10, height=10, units="cm")
# conduct z-tests on each region and print output
# --------------------------------------------------------------------------------
reg = c("IPS", "Put", "SMA")
lapply(reg, function(x) with(dat, z.test(std[hemisphere == "left" & reg == x],
std[hemisphere == "right" & reg == x],
x)))
# s1 data = IPS: z = -0.2, p = .844, Put: z = 0.12, p = .91, SMA: z = .02, p = .98
# s2_SING = IPS: z = -0.3, p = .78, Put: z = .05, p = .96, SMA: z = -0.01, p = .99
# s2_MULT = IPS: z = -0.28, p = .77, Put: z = .05, p = .96, SMA: z = -0.04, p = .97
