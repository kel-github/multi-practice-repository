#   prior.dat$task.B[prior.dat$trial==i] = task.b.priors
kl.dat$KL[i] = get.KL(task.b.priors, task.a.priors)
kl.dat$interference.T[i] = 1/kl.dat$KL[i]
}
kl.dat
}
n.stim = 10
n.task = 2
n.features = 10 # randomly chosen
n.practice.trials = 15
priors = rep(1, times=(n.stim*n.task)+n.stim) # priors over each the task space (to define Dirichlet distribution)
learn.rate = 0.01 # a weighting for new observations
cov.rngs = list(lo=c(-1, 1),hi=c(9, 11))
cov.fact=c("lo", "hi")
n.vars = length(cov.rngs)
n.sims = 1000 # number of simulations to perform
cov.mat = lapply(cov.rngs, gen.cov.mat, n.stim=(n.stim*n.task)+n.stim) # generate a covariance matrix from which to generate random, correlated vectors that is n = total stim across tasks, plus n.stim number of coincidental stimuli
cov.mat = lapply(cov.mat, make.positive.definite, tol=1e-3) # using this to make matrix positive definite, on account of advice from https://stackoverflow.com/questions/27176595/error-sigma-must-be-positive-definite/27179588
# if required, make positive definite
s.mat = lapply(cov.mat, mvrnorm, n=n.features, mu=rep(0, times=(n.stim*n.task)+n.stim))
# check the stim.mat covariance matrix is similar to the input one
check = cov(s.mat$lo) # visual check is comforting
# now normalise the vectors to unit length (for using distance measure below)
s.mat = lapply(s.mat, function(x) apply(x, 2, norm.vec.lngth))
# first, generate the stimulus based on one of the existing stimulus vectors, add noise
this.stim.idx = sample(1:n.stim*n.task, 1) # which stimulus is presented?
stim.encoded = s.mat$hi[,this.stim.idx] + rnorm(n.features, mean=0, sd=.1)
stim.encoded = norm.vec.lngth(stim.encoded)
distance = apply(s.mat$hi, 2, calc.euclid, y = stim.encoded)
similarity = 1/distance
kl.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task, n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features, learn.rate=learn.rate, stim.overlap = "none"), SIMPLIFY=FALSE))
kl.dat = do.call(rbind, kl.dat)
kl.dat = kl.dat %>% group_by(covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
kl.dat
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 4000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 5000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
overlap.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "low"), SIMPLIFY=FALSE))
overlap.dat = do.call(rbind, overlap.dat)
overlap.dat$overlap = "low"
nu.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "high"), SIMPLIFY=FALSE))
nu.dat = do.call(rbind, nu.dat)
nu.dat$overlap = "high"
overlap.dat = rbind(overlap.dat, nu.dat)
rm(nu.dat)
overlap.dat$overlap <- as.factor(overlap.dat$overlap)
levels(overlap.dat$overlap) = c("low", "high")
overlap.dat = overlap.dat %>% group_by(overlap, covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
overlap.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1.5) +
ylim(10, 5000) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.4, colour=NA) +
facet_wrap(~ overlap) +
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
rm(list=ls())
library(tidyverse)
library(rmarkdown)    # You need this library to run this template.
library(epuRate)      # Install with devtools: install_github("holtzy/epuRate", force=TRUE)
library(MASS)
library(corpcor)
library(wesanderson)
gen.cov.mat <- function(n.stim, cov.rng){
# generates covariance matrix to initialise vectors
# output = mat (an n.stim x n.stim covariance matrix)
# inputs =
# n.stim - the number of stimuli, covariance matrix will be n x n
# cor.rng = lower and upper bound of task covariances
mat = diag(x=1, n.stim, n.stim)
cor.rng <- runif(length(mat[lower.tri(mat)]), min(cov.rng), max(cov.rng))
mat[lower.tri(mat)] = cor.rng
mat = t(mat)
mat[lower.tri(mat)] = cor.rng
mat
}
norm.vec.lngth <- function(v){
# this function takes a vector and returns the unit vector
mu = 1/ sqrt( t(v) %*% v ) # first is transferred as inputs are interpreted as row vectors
v = mu * v
v
}
calc.euclid <- function(x,y){
# compute the euclidean distance between two vectors
sqrt( sum( (x-y) ^ 2 )  )
}
rdirichlet<-function (n, alpha) {
# I got this function from:
# https://stats.stackexchange.com/questions/166289/generate-data-from-dirichlet-distribution - also see page 585 of Gelman's BDA, 3r ed.
l <- length(alpha)
x <- matrix(rgamma(l * n, alpha), ncol = l, byrow = TRUE) # generate n observations using the gamma function, given l alpha paramaters, and n observations
sm <- x %*% rep(1, l) # sum each row of the matrix
x/as.vector(sm)
}
get.KL <- function(alpha, beta){
# find out how well the dirichlet distribution defined by alpha is approximated by the dirichlet distribution defined by the beta parameters. Computes the KL divergence using the derivation at:
# http://bariskurt.com/kullback-leibler-divergence-between-two-dirichlet-and-beta-distributions/
lgamma(sum(alpha)) - lgamma(sum(beta)) - sum(lgamma(alpha)) + sum(lgamma(beta)) + sum((alpha - beta)*(digamma(alpha)-digamma(sum(beta))))
}
# get.interference <- function(p){
#   # implement Hick-Hyman's law for a decision involving 2 events, p = p of event 1
#   sum(  p * log2( (1/p) + 1 ),  (1-p) * log2( (1/( 1- p)) + 1   ) )
# }
generate.noise <- function(n.feat, n.stim){
# n.feat = the number of features that require noise assigned to them
# n.stim = the total number of stimuli sampled across the two tasks
# mat.dims = the dimensions of the n x p internal stimulus matrix
# output is a matrix of noise to be added to the stimuli encoded matrix
matrix(rnorm(n.feat*n.stim, mean=0, sd=.1), nrow=n.feat)
}
run.sim <- function(n.stim, n.task, n.vars, n.practice.trials, priors, stim.mat, n.features, learn.rate, cov.fact, stim.overlap){
# initialise dataframe to collect behavioural results
kl.dat = data.frame(covar=factor(rep(cov.fact, each=n.practice.trials)),
trial=factor(c(1:n.practice.trials)),
KL=rep(NA, times=n.practice.trials),
interference.T=rep(NA, times=n.practice.trials))
task.a.priors = priors # initialise task A model parameters
task.b.priors = priors  # same for task B
# set the range of values to sample stimuli from for each task
irrel.idxs = ((n.task*n.stim)+1):ncol(stim.mat)
task.idxs = matrix(c(1:(n.stim*n.task)), byrow=FALSE, nrow=n.stim)
if (stim.overlap == "low") {
task.idxs[sample(1:nrow(task.idxs), 5),1] = task.idxs[sample(1:nrow(task.idxs), 5), 2]
} else if (stim.overlap == "high") {
task.idxs[,1] = task.idxs[, 2]
}
# now run the simulation
for (i in 1:n.practice.trials){
# step 1: select stimuli from the set of task A, from the set of task B, and if an even trial, from the set of task C
if (stim.overlap == "none"){
n = 1
} else {
n = 2
}
trial.idxs = apply(task.idxs, 2, sample, n)
# if an even trial, add a coincidental stimulus
if (i%%2 == 0) trial.idxs = rbind(trial.idxs, rep(sample(irrel.idxs, 1), times=n.task))
trial.idxs = as.vector(trial.idxs)
# encoded stimulu
stim.encoded = stim.mat[,trial.idxs] + generate.noise(n.features, length(trial.idxs))
stim.encoded = apply(stim.encoded, 2, norm.vec.lngth)
distance = apply(stim.encoded, 2, function(x) apply(stim.mat, 2, calc.euclid, y = x))
similarity = 1/distance
# update priors (from 1st half of sim mat columns for task A, second half for task B)
if (n.task == 2 & i%%2 != 0 & stim.overlap == "none"){
task.a.priors = task.a.priors + (learn.rate * similarity[,1])
task.b.priors = task.b.priors + (learn.rate * similarity[,2])
} else {
task.a.priors = task.a.priors + (learn.rate * apply(similarity[,1:(length(trial.idxs)/2)], 1, sum ))
task.b.priors = task.b.priors + (learn.rate * apply(similarity[,((length(trial.idxs)/2)+1):length(trial.idxs)], 1, sum ))
}
#   prior.dat$task.A[prior.dat$trial==i] = task.a.priors
#   prior.dat$task.B[prior.dat$trial==i] = task.b.priors
kl.dat$KL[i] = get.KL(task.b.priors, task.a.priors)
kl.dat$interference.T[i] = 1/kl.dat$KL[i]
}
kl.dat
}
n.stim = 10
n.task = 2
n.features = 10 # randomly chosen
n.practice.trials = 10
priors = rep(1, times=(n.stim*n.task)+n.stim) # priors over each the task space (to define Dirichlet distribution)
learn.rate = 0.01 # a weighting for new observations
cov.rngs = list(lo=c(-1, 1),hi=c(9, 11))
cov.fact=c("lo", "hi")
n.vars = length(cov.rngs)
n.sims = 1000 # number of simulations to perform
cov.mat = lapply(cov.rngs, gen.cov.mat, n.stim=(n.stim*n.task)+n.stim) # generate a covariance matrix from which to generate random, correlated vectors that is n = total stim across tasks, plus n.stim number of coincidental stimuli
cov.mat = lapply(cov.mat, make.positive.definite, tol=1e-3) # using this to make matrix positive definite, on account of advice from https://stackoverflow.com/questions/27176595/error-sigma-must-be-positive-definite/27179588
# if required, make positive definite
s.mat = lapply(cov.mat, mvrnorm, n=n.features, mu=rep(0, times=(n.stim*n.task)+n.stim))
# check the stim.mat covariance matrix is similar to the input one
check = cov(s.mat$lo) # visual check is comforting
# now normalise the vectors to unit length (for using distance measure below)
s.mat = lapply(s.mat, function(x) apply(x, 2, norm.vec.lngth))
# first, generate the stimulus based on one of the existing stimulus vectors, add noise
this.stim.idx = sample(1:n.stim*n.task, 1) # which stimulus is presented?
stim.encoded = s.mat$hi[,this.stim.idx] + rnorm(n.features, mean=0, sd=.1)
stim.encoded = norm.vec.lngth(stim.encoded)
distance = apply(s.mat$hi, 2, calc.euclid, y = stim.encoded)
similarity = 1/distance
kl.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task, n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features, learn.rate=learn.rate, stim.overlap = "none"), SIMPLIFY=FALSE))
kl.dat = do.call(rbind, kl.dat)
kl.dat = kl.dat %>% group_by(covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 5000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6000) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
overlap.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "low"), SIMPLIFY=FALSE))
overlap.dat = do.call(rbind, overlap.dat)
overlap.dat$overlap = "low"
nu.dat = replicate(n.sims, mapply(stim.mat=s.mat, cov.fact=cov.fact, run.sim, MoreArgs=list(n.stim=n.stim, n.task=n.task,
n.vars=n.vars, n.practice.trials=n.practice.trials, priors=priors, n.features=n.features,
learn.rate=learn.rate, stim.overlap = "high"), SIMPLIFY=FALSE))
nu.dat = do.call(rbind, nu.dat)
nu.dat$overlap = "high"
overlap.dat = rbind(overlap.dat, nu.dat)
rm(nu.dat)
overlap.dat$overlap <- as.factor(overlap.dat$overlap)
levels(overlap.dat$overlap) = c("low", "high")
overlap.dat = overlap.dat %>% group_by(overlap, covar, trial) %>%
summarise(multicost = mean(interference.T),
se=sd(interference.T)/sqrt(length(interference.T)))
overlap.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1.5) +
ylim(10, 6000) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.4, colour=NA) +
facet_wrap(~ overlap) +
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
warnings()
kl.dat %>%
ggplot(aes(trial, multicost, color=covar, group=covar)) +
geom_line(size=1) +
scale_color_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
scale_fill_manual(values=wesanderson::wes_palette("IsleofDogs1")[c(1:2)]) +
geom_ribbon(aes(ymin=multicost-(1.96*se), ymax=multicost+(1.96*se)), alpha=0.3, colour=NA) +
ylim(10, 6500) + # a range that is typical across all sims
theme_classic() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank()) +
labs(colour = "Covariance",
x = "trials [au]",
y = "multitasking cost [au]")
randperm(3)
sample(3)
sample(3,1)
rm(list=ls())
# 6. Print hello world
# ------------------------
print('hello world')
rm(list=ls())
# load required packages and plotting resources
# --------------------------------------------------------------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(cowplot)
library(wesanderson)
library(BayesFactor)
library(rstatix)
source("../s2n_analysis/R_rainclouds.R")
# define functions
# --------------------------------------------------------------------------------
get.data <- function(csv_file, data_path, conds, excl){
# csv_file = csv file name
# data_path = folder name of csv file
# conds = 2 element vector of the two condition names
data <- read.csv(paste('../', data_path, '/', csv_file, sep=""), header=F)
names(data) <- c("IPL", "Put", "SMA", "sub", "cond")
data$group <- NA
data$group[data$sub < 200] <- "practice"
data$group[data$sub > 199] <- "control"
data = data %>% filter(!(sub %in% excl)) # remove excl subs
# label factors
data$sub <- as.factor(data$sub)
for (i in 1:length(conds))  data$cond[data$cond == i] <- conds[i]
data$cond <- as.factor(data$cond)
data$group <- factor(data$group, levels=c("practice", "control"))
data
}
get.cor.results <- function(data, pair, name){
data %>% filter(region %in% pair) %>%
group_by(sub, cond) %>%
summarise(group=group[1],
regions=name,
r=cor(y[region==pair[1]], y[region==pair[2]]))
}
plot.cor.results <- function(data, title){
summary_data <- data %>% group_by(group, regions, cond) %>%
summarise(mean=mean(r),
N=length(r),
se=sd(r)/sqrt(N))
# ggplot(data, aes(x=regions, y=r, fill = cond, colour = cond)) +
#   geom_flat_violin(aes(fill=cond), position = position_nudge(x = .25, y = 0), adjust=1.5, trim =
#                      FALSE, alpha = 0.5) +
#   geom_point(aes(x=regions, y=r, colour=cond), position = position_jitter(width = .15), size = .25) +
#   geom_boxplot(aes(x = regions, y = r, fill=cond), outlier.shape = NA,
#                alpha = 0.3, width = .15, colour = "BLACK") +
#   scale_color_manual(values=wes_palette("Cavalcanti1")) +
#   scale_fill_manual(values=wes_palette("Cavalcanti1")) +
#   ylab('r') + xlab('region pairing') + theme_cowplot() +
#   facet_wrap(~group, nrow=2) +
#   theme(axis.text.x = element_text(face = "italic")) +
#   ggtitle(title)
ggplot(data, aes(x=regions, y=r, fill = cond, colour = cond)) +
geom_flat_violin(aes(fill=cond), position = position_nudge(x = .25, y = 0), adjust=1.5, trim =
FALSE, alpha = 0.5) +
geom_point(aes(x=regions, y=r, colour=cond), position = position_jitter(width = .15), size = .25) +
geom_point(data=summary_data, aes(x = regions, y = mean), position=position_nudge(.25), colour = "BLACK") +
scale_color_manual(values=wes_palette("Cavalcanti1")) +
scale_fill_manual(values=wes_palette("Cavalcanti1")) +
ylab('r') + xlab('region pairing') + theme_cowplot() +
facet_wrap(~group, nrow=2) +
theme(axis.text.x = element_text(face = "italic")) +
ggtitle(title)
}
# define file variables
# --------------------------------------------------------------------------------
s1.LH.excl = c(102, 128, 138, 203)
s1.RH.excl = c(128, 203)
s2.LH.SING.excl = c(102, 128, 138, 144, 203)
s2.RH.SING.excl = c(102, 106, 128, 138, 144, 203)
s2.LH.MULT.excl = c(102, 128, 138, 144, 203)
s2.RH.MULT.excl = c(102, 104, 106, 128, 138, 144, 203)
data.dir = 'processed-data'
dat.csv = 's2_RH_MULT_tSERIES.csv'
roi_names = c("IPS", "Put", "SMA")
conds = c("pre", "post")
save.name = 'fc-s2-RH_MULT'
excl = s2.RH.MULT.excl
plot_title = "right: pre-post MT"
session = 2
###########################################################################################
# RUN CODE
data <- get.data(csv_file=dat.csv, data_path=data.dir, conds=conds, excl=excl) %>%
pivot_longer(c('IPL', 'Put', 'SMA'), names_to = "region", values_to="y")
# if s1 run this code
data$cond <- factor(data$cond, levels=conds)
# run correlation by reg
reg.pair <- list(c("IPL", "Put"), c("Put", "SMA"), c("IPL", "SMA"))
reg.pair.name <- list(c("IPL-Put"), c("Put-SMA"), c("IPL-SMA"))
cor.results <- do.call(rbind, (mapply(get.cor.results, pair=reg.pair, name=reg.pair.name, MoreArgs=list(data=data), SIMPLIFY=FALSE)))
cor.results$regions <- as.factor(cor.results$regions)
p <- plot.cor.results(cor.results, title=plot_title)
p
## written by K. Garner, 2020
# this is to conduct and plot a basic FC analysis, from the regions
# of interest in the left and right hemipheres of Garner et al 2020 https://doi.org/10.1101/564450
# as requested by the editor of eNeuro
rm(list=ls())
# load required packages and plotting resources
# --------------------------------------------------------------------------------
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(cowplot)
library(wesanderson)
library(BayesFactor)
library(rstatix)
source("../s2n_analysis/R_rainclouds.R")
# define functions
# --------------------------------------------------------------------------------
get.data <- function(csv_file, data_path, conds, excl){
# csv_file = csv file name
# data_path = folder name of csv file
# conds = 2 element vector of the two condition names
data <- read.csv(paste('../', data_path, '/', csv_file, sep=""), header=F)
names(data) <- c("IPL", "Put", "SMA", "sub", "cond")
data$group <- NA
data$group[data$sub < 200] <- "practice"
data$group[data$sub > 199] <- "control"
data = data %>% filter(!(sub %in% excl)) # remove excl subs
# label factors
data$sub <- as.factor(data$sub)
for (i in 1:length(conds))  data$cond[data$cond == i] <- conds[i]
data$cond <- as.factor(data$cond)
data$group <- factor(data$group, levels=c("practice", "control"))
data
}
get.cor.results <- function(data, pair, name){
data %>% filter(region %in% pair) %>%
group_by(sub, cond) %>%
summarise(group=group[1],
regions=name,
r=cor(y[region==pair[1]], y[region==pair[2]]))
}
plot.cor.results <- function(data, title){
summary_data <- data %>% group_by(group, regions, cond) %>%
summarise(mean=mean(r),
N=length(r),
se=sd(r)/sqrt(N))
ggplot(data, aes(x=regions, y=r, fill = cond, colour = cond)) +
geom_flat_violin(aes(fill=cond), position = position_nudge(x = .25, y = 0), adjust=1.5, trim =
FALSE, alpha = 0.5) +
geom_point(aes(x=regions, y=r, colour=cond), position = position_jitter(width = .15), size = .25) +
geom_boxplot(aes(x = regions, y = r, fill=cond), outlier.shape = NA,
alpha = 0.3, width = .05, colour = "BLACK") +
scale_color_manual(values=wes_palette("Cavalcanti1")) +
scale_fill_manual(values=wes_palette("Cavalcanti1")) +
ylab('r') + xlab('region pairing') + theme_cowplot() +
facet_wrap(~group, nrow=2) +
theme(axis.text.x = element_text(face = "italic")) +
ggtitle(title)
}
# define file variables
# --------------------------------------------------------------------------------
s1.LH.excl = c(102, 128, 138, 203)
s1.RH.excl = c(128, 203)
s2.LH.SING.excl = c(102, 128, 138, 144, 203)
s2.RH.SING.excl = c(102, 106, 128, 138, 144, 203)
s2.LH.MULT.excl = c(102, 128, 138, 144, 203)
s2.RH.MULT.excl = c(102, 104, 106, 128, 138, 144, 203)
data.dir = 'processed-data'
dat.csv = 's2_RH_MULT_tSERIES.csv'
roi_names = c("IPS", "Put", "SMA")
conds = c("pre", "post")
save.name = 'fc-s2-RH_MULT'
excl = s2.RH.MULT.excl
plot_title = "right: pre-post MT"
session = 2
###########################################################################################
# RUN CODE
data <- get.data(csv_file=dat.csv, data_path=data.dir, conds=conds, excl=excl) %>%
pivot_longer(c('IPL', 'Put', 'SMA'), names_to = "region", values_to="y")
# if s1 run this code
data$cond <- factor(data$cond, levels=conds)
# run correlation by reg
reg.pair <- list(c("IPL", "Put"), c("Put", "SMA"), c("IPL", "SMA"))
reg.pair.name <- list(c("IPL-Put"), c("Put-SMA"), c("IPL-SMA"))
cor.results <- do.call(rbind, (mapply(get.cor.results, pair=reg.pair, name=reg.pair.name, MoreArgs=list(data=data), SIMPLIFY=FALSE)))
cor.results$regions <- as.factor(cor.results$regions)
p <- plot.cor.results(cor.results, title=plot_title)
p
